{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exrRgKyl5ihO",
        "outputId": "563d3d5b-23fe-47c1-fa0d-02a8d478e61a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface-hub python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1"
      ],
      "metadata": {
        "id": "SosXlqvP9fqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "def initialize_client():\n",
        "    \"\"\"Initializes and returns the Hugging Face InferenceClient.\"\"\"\n",
        "    token = \"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\"\n",
        "    if not token:\n",
        "        raise ValueError(\"Hugging Face token not found. Please set it in your .env file.\")\n",
        "    # No need to specify the model here if we pass it in the call\n",
        "    return InferenceClient(token=token)\n",
        "\n",
        "# --- THIS FUNCTION HAS BEEN UPDATED ---\n",
        "def get_text_completion(client, prompt, max_new_tokens=250, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Sends a prompt to the Hugging Face API using the chat completion task\n",
        "    and returns the response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use client.chat_completion for conversational models\n",
        "        response_stream = client.chat_completion(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}], # Format input as a chat message\n",
        "            model=MODEL_ID, # Specify the model here\n",
        "            max_tokens=max_new_tokens, # Note: parameter is max_tokens for this method\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            stream=False # Set to False to get the full response at once\n",
        "        )\n",
        "\n",
        "        # The response object has a different structure now\n",
        "        return response_stream.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle cases where the model might be loading (503 error) or other issues\n",
        "        return f\"An API error occurred: {e}\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the text completion application.\"\"\"\n",
        "    try:\n",
        "        client = initialize_client()\n",
        "        print(f\"AI Text Completion App (Model: {MODEL_ID})\")\n",
        "        print(\"Enter 'exit' or 'quit' to end the session.\")\n",
        "\n",
        "        while True:\n",
        "            user_prompt = input(\"\\nEnter your prompt: \")\n",
        "\n",
        "            if user_prompt.lower() in ['exit', 'quit']:\n",
        "                print(\"Exiting the application. Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if not user_prompt.strip():\n",
        "                print(\"Input cannot be empty. Please enter a prompt.\")\n",
        "                continue\n",
        "\n",
        "            print(\"\\nGenerating response...\")\n",
        "\n",
        "            completion = get_text_completion(client, user_prompt)\n",
        "\n",
        "            print(\"\\n--- AI Response ---\")\n",
        "            print(completion)\n",
        "            print(\"-------------------\\n\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"Initialization Error: {ve}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "atHn35_i6I2d",
        "outputId": "0f62e937-40c4-425d-b02a-17e463449442"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Text Completion App (Model: mistralai/Mistral-7B-Instruct-v0.2)\n",
            "Enter 'exit' or 'quit' to end the session.\n",
            "\n",
            "Enter your prompt: Write a haiku about the ocean\n",
            "\n",
            "Generating response...\n",
            "\n",
            "--- AI Response ---\n",
            " Endless waves dance,\n",
            "Salty breeze whispers secrets,\n",
            "Peace in ocean's heart.\n",
            "-------------------\n",
            "\n",
            "\n",
            "Enter your prompt: Once upon a time, there was a robot who…\n",
            "\n",
            "Generating response...\n",
            "\n",
            "--- AI Response ---\n",
            " Once upon a time, in a bustling city filled with towering skyscrapers and neon lights, there was a robot named ASIMO, short for Advanced Smart Interactive Mobile robot. ASIMO was created by Honda Motor Co. with the goal of being a helpful companion and assistant to humans.\n",
            "\n",
            "ASIMO was unlike any other robot in existence. He was agile, able to walk, run, and even dance. He could understand and respond to human speech, recognize faces, and even mimic human emotions. ASIMO was designed to make life easier for people, whether it was by carrying heavy loads, helping with household chores, or providing companionship to the elderly or disabled.\n",
            "\n",
            "Despite being made of metal and circuits, ASIMO had a warm and friendly demeanor that put people at ease. He was always eager to learn new things and help out in any way he could. ASIMO quickly became a beloved member of the community, bringing joy and assistance to everyone he met.\n",
            "\n",
            "One sunny afternoon, ASIMO was out for a stroll in the park, enjoying the fresh air and sunshine. He saw a group of children playing\n",
            "-------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-725834667.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-6-725834667.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0muser_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter your prompt: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muser_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2 and 3"
      ],
      "metadata": {
        "id": "8wwVvae89jYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Model Configuration\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "def initialize_client():\n",
        "    \"\"\"Initializes and returns the Hugging Face InferenceClient with a timeout.\"\"\"\n",
        "    token = \"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\"\n",
        "    if not token:\n",
        "        raise ValueError(\"Hugging Face token not found. Please set it in your .env file.\")\n",
        "    # Add a timeout to handle connection issues\n",
        "    return InferenceClient(token=token, timeout=30)\n",
        "\n",
        "def get_chat_response(client, prompt, max_tokens, temperature, top_p):\n",
        "    \"\"\"\n",
        "    Sends a prompt to the Hugging Face API using the chat completion task\n",
        "    and returns the response.\n",
        "    \"\"\"\n",
        "    # This standard list of dictionaries is the correct way.\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    try:\n",
        "        response = client.chat_completion(\n",
        "            messages=messages,\n",
        "            model=MODEL_ID,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            stream=False\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        if \"503\" in str(e):\n",
        "            return f\"An API error occurred: The model {MODEL_ID} is currently loading. Please try again in a few moments.\"\n",
        "        return f\"An API error occurred: {e}\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run an enhanced text completion application.\"\"\"\n",
        "    try:\n",
        "        client = initialize_client()\n",
        "        print(f\"--- AI Text Completion App (Model: {MODEL_ID}) ---\")\n",
        "        print(\"Special Commands:\")\n",
        "        print(\"  set temp=<value>    (e.g., set temp=0.8)\")\n",
        "        print(\"  set tokens=<value>  (e.g., set tokens=300)\")\n",
        "        print(\"  set top_p=<value>   (e.g., set top_p=0.9)\")\n",
        "        print(\"  Enter 'exit' or 'quit' to end the session.\")\n",
        "        print(\"--------------------------------------------------\")\n",
        "\n",
        "        # --- Default Response Parameters ---\n",
        "        params = {\n",
        "            \"temperature\": 0.7,\n",
        "            \"max_tokens\": 250,\n",
        "            \"top_p\": 0.9,\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            # Display current settings in the prompt\n",
        "            current_settings = f\"temp={params['temperature']}, tokens={params['max_tokens']}, top_p={params['top_p']}\"\n",
        "            user_input = input(f\"\\nEnter your prompt [{current_settings}]: \")\n",
        "\n",
        "            # Command and Input Handling\n",
        "            if user_input.lower() in ['exit', 'quit']:\n",
        "                print(\"Exiting the application. Goodbye!\")\n",
        "                break\n",
        "\n",
        "            # --- Handle Parameter Changes ---\n",
        "            if user_input.lower().startswith(\"set \"):\n",
        "                try:\n",
        "                    parts = user_input.split(\"=\")\n",
        "                    command = parts[0].lower().strip().split(\" \")[1]\n",
        "                    value = parts[1].strip()\n",
        "\n",
        "                    if command == \"temp\":\n",
        "                        params['temperature'] = float(value)\n",
        "                        print(f\"Temperature set to {params['temperature']}\")\n",
        "                    elif command == \"tokens\":\n",
        "                        params['max_tokens'] = int(value)\n",
        "                        print(f\"Max tokens set to {params['max_tokens']}\")\n",
        "                    elif command == \"top_p\":\n",
        "                        params['top_p'] = float(value)\n",
        "                        print(f\"Top P set to {params['top_p']}\")\n",
        "                    else:\n",
        "                        print(\"Unknown command. Use 'temp', 'tokens', or 'top_p'.\")\n",
        "                except (IndexError, ValueError):\n",
        "                    print(\"Invalid format. Use 'set <param>=<value>' (e.g., 'set temp=0.5').\")\n",
        "                continue\n",
        "\n",
        "            # --- Handle Edge Cases for Input ---\n",
        "            if not user_input.strip():\n",
        "                print(\"Input cannot be empty. Please enter a prompt.\")\n",
        "                continue\n",
        "\n",
        "            if len(user_input) > 4000:\n",
        "                print(\"Warning: Your input is very long. This may result in slower response times or errors.\")\n",
        "\n",
        "            print(\"\\nGenerating response...\")\n",
        "\n",
        "            completion = get_chat_response(client, user_input, **params)\n",
        "\n",
        "            print(\"\\n--- AI Response ---\")\n",
        "            print(completion)\n",
        "            print(\"-------------------\\n\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"Initialization Error: {ve}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBhZFtO199Cz",
        "outputId": "c9a236aa-1ed2-4072-8dd7-ebb1ecdd9d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- AI Text Completion App (Model: mistralai/Mistral-7B-Instruct-v0.2) ---\n",
            "Special Commands:\n",
            "  set temp=<value>    (e.g., set temp=0.8)\n",
            "  set tokens=<value>  (e.g., set tokens=300)\n",
            "  set top_p=<value>   (e.g., set top_p=0.9)\n",
            "  Enter 'exit' or 'quit' to end the session.\n",
            "--------------------------------------------------\n",
            "\n",
            "Enter your prompt [temp=0.7, tokens=250, top_p=0.9]: Continue this story of a bear going to a market: Hugs the bear went to the market on a fine tuesday morning\n",
            "\n",
            "Generating response...\n",
            "\n",
            "--- AI Response ---\n",
            " Hugs the bear, with a small basket slung over his broad shoulder, ambled through the sun-dappled forest, making his way towards the bustling market square. The scent of fresh bread and sizzling sausages wafted through the air, mingling with the fragrant blooms of wildflowers and the crisp, earthy aroma of the forest.\n",
            "\n",
            "As he approached the market, Hugs could hear the lively chatter and laughter of vendors and shoppers alike. He smiled to himself, feeling a sense of excitement and anticipation. This was always his favorite part of the week – a chance to trade the quiet solitude of the forest for the vibrant energy of the marketplace.\n",
            "\n",
            "The bear lumbered through the crowd, greeting old friends and making new acquaintances along the way. The market vendors welcomed him with warm smiles and hearty pats on the back, always eager to see their friendly forest friend. Hugs browsed the stalls, sampling delicious fruits, crunchy vegetables, and fragrant spices, chatting and laughing with the vendors as he went.\n",
            "\n",
            "At one stall, he met a particularly charming vendor\n",
            "-------------------\n",
            "\n",
            "\n",
            "Enter your prompt [temp=0.7, tokens=250, top_p=0.9]: Create a summary of the following text: In astronomy, dark matter is an invisible and hypothetical form of matter that does not interact with light or other electromagnetic radiation. Dark matter is implied by gravitational effects that cannot be explained by general relativity unless more matter is present than can be observed. Such effects occur in the context of formation and evolution of galaxies,[1] gravitational lensing,[2] the observable universe's current structure, mass position in galactic collisions,[3] the motion of galaxies within galaxy clusters, and cosmic microwave background anisotropies. Dark matter is thought to serve as gravitational scaffolding for cosmic structures.[4] After the Big Bang, dark matter clumped into blobs along narrow filaments with superclusters of galaxies forming a cosmic web at scales on which entire galaxies appear like tiny particles.[5][6]  In the standard Lambda-CDM model of cosmology, the mass–energy content of the universe is 5% ordinary matter, 26.8% dark matter, and 68.2% a form of energy known as dark energy.[7][8][9][10] Thus, dark matter constitutes 85% of the total mass, while dark energy and dark matter constitute 95% of the total mass–energy content.[11][12][13][14] While the density of dark matter is significant in the halo around a galaxy, its local density in the Solar System is much less than normal matter. The total of all the dark matter out to the orbit of Neptune would add up about 1017 kg, the same as a large asteroid.[15]\n",
            "\n",
            "Generating response...\n",
            "\n",
            "--- AI Response ---\n",
            " Dark matter is an unobserved form of matter in astronomy that doesn't interact with light or electromagnetic radiation. Its presence is inferred through gravitational effects on the formation and evolution of galaxies, gravitational lensing, the structure of the observable universe, and other phenomena. In the standard Lambda-CDM model of cosmology, dark matter makes up about 27% of the total mass in the universe, while dark energy accounts for around 68%, with only 5% being ordinary matter. Despite its significant cosmic role, the local density of dark matter in our Solar System is much less than normal matter, with the total amount within Neptune's orbit being equivalent to a large asteroid.\n",
            "-------------------\n",
            "\n",
            "\n",
            "Enter your prompt [temp=0.7, tokens=250, top_p=0.9]: Explain recursion like I’m five.\n",
            "\n",
            "Generating response...\n",
            "\n",
            "--- AI Response ---\n",
            " Recursion is like telling a really good story, but instead of it having an ending, it keeps going back to the beginning and adding something new each time. So, you start with a simple step, like \"Once upon a time, there was a frog,\" and then you add something new, like \"This frog could jump really high.\" And then, when you want to talk about how high this frog could jump, you tell the same story about that jump being really high, but this time the frog is a little froggy friend of the first frog. And so on, until you've told the story of all the frogs who can jump high! It's a way for a computer to solve a problem by breaking it down into smaller and smaller pieces, and each piece is solved by telling the same thing over and over again, but with a little twist each time.\n",
            "-------------------\n",
            "\n",
            "\n",
            "Enter your prompt [temp=0.7, tokens=250, top_p=0.9]: Tell me the history of crypto\n",
            "\n",
            "Generating response...\n",
            "\n",
            "--- AI Response ---\n",
            " Cryptocurrency, specifically Bitcoin, was first introduced to the world in a whitepaper published on October 31, 2008, under the pseudonym Satoshi Nakamoto. The paper outlined a new digital currency that would use cryptography for security and operate on a decentralized network. On January 3, 2009, the first Bitcoin block, called the genesis block, was mined, marking the beginning of the Bitcoin network.\n",
            "\n",
            "The early years of cryptocurrency were marked by a small but dedicated community of users and miners. Bitcoin's price was negligible, and its use was primarily limited to online forums and dark web marketplaces. One of the most notable early transactions was the purchase of two pizzas for 10,000 Bitcoins in May 2010. At the time, the transaction was worth approximately $41. Today, that same amount of Bitcoin is worth over $400 million.\n",
            "\n",
            "As more people began to take notice of Bitcoin and other cryptocurrencies, the price began to rise. In 2011, Bitcoin reached an all-time high of $\n",
            "-------------------\n",
            "\n",
            "\n",
            "Enter your prompt [temp=0.7, tokens=250, top_p=0.9]: Write a short job description for a construction site manager\n",
            "\n",
            "Generating response...\n",
            "\n",
            "--- AI Response ---\n",
            " Job Title: Construction Site Manager\n",
            "\n",
            "Job Summary:\n",
            "We are seeking an experienced and skilled Construction Site Manager to join our team. The ideal candidate will have a strong background in construction project management, excellent leadership abilities, and a proven track record of delivering projects on time and within budget. The Construction Site Manager will be responsible for overseeing all aspects of construction projects from planning and preparation through to completion.\n",
            "\n",
            "Key Responsibilities:\n",
            "- Managing and coordinating construction activities, including scheduling, budgeting, and resource allocation\n",
            "- Ensuring projects are completed on time, within budget, and in accordance with all relevant regulations and quality standards\n",
            "- Leading and managing a team of construction workers and subcontractors\n",
            "- Developing and implementing safety protocols to ensure a safe working environment\n",
            "- Coordinating with architects, engineers, and other stakeholders to ensure projects meet design specifications\n",
            "- Managing project documentation, including progress reports, change orders, and project closeout reports\n",
            "- Addressing and resolving any issues that arise during construction\n",
            "\n",
            "Qualifications:\n",
            "- Minimum of 5 years of experience in construction project management or a related field\n",
            "- Proven experience managing construction projects from start to\n",
            "-------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A4sJ39uxCrAN"
      }
    }
  ]
}